# -*- coding: utf-8 -*-
"""regression_task.ipynb

Automatically generated by Colaboratory.

Name: Rohan Shrestha

##**MultiVariate Linear Regression**

From the name itself, we can find that in this supervised machine learning algorithm we wil be working with multiple variables. By saying variables, the independent variables will be multiple in number in their own separate column.
This algorithm is also sometimes coined as Multiple Linear Regression
"""

# mounting google drive with colaboratory to access the csv file we are going to work on
from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# plots and diagrams will be stored within the notebook
# %matplotlib inline

"""##**Importing all the neccessary libraries to for the regression calculation**"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

plt.rcParams['figure.figsize'] = (10.0, 10.0)  # setting the size of plots

# reading data from csv file and storing in 'data'
data = pd.read_csv('/content/drive/MyDrive/Melbourne_housing.csv')

# displaying first five rows of data
data.head()

"""This dataset is the housing data of Mebourne, Australia.
Here, the first column suburb holds the different city/district names

2nd Column holds the addresses of the houses in Melbourne

3rd column holds the no. of rooms in each of the houses

4th column holds the house type

5th column holds the Price of each Melbourne houses

Similarly, we won't be working with the 6th 7th and 8th column as they hold non-nominal/categorical values

Whereas rest of the remaining columns will be used to build the regression model.
"""

data.info()

"""From the above info we observe that even though the some of the variables hold integer values but their data type is being shown as object type."""

# dropping the categorical variables
data.drop(['Suburb', 'Address', 'Method', 'SellerG', 'Date'], inplace = True, axis = 1)
# (starting from 0) the 386th row in the dataset contains the Title value once again, which will interrupt in the regression calculation
# Therefore, dropping that respective row from the dataset.
data.drop(labels = 386, inplace = True, axis =0)

# printing the columns after dropping the other columns
data.head()

data.tail()

data.isnull()  # checking if any of the value is null or NaN.

"""Now here, we are using the label encoder function from the sklearn library in order to map all of the object types into integer types to perform calculations"""

from sklearn.preprocessing import LabelEncoder
label = LabelEncoder()
data.Type = label.fit_transform(data.Type)
data.Rooms = label.fit_transform(data.Rooms)
data.Price = label.fit_transform(data.Price)
data.Distance = label.fit_transform(data.Distance)
data.Week = label.fit_transform(data.Week)
data.CPI = label.fit_transform(data.CPI)
data.LastCPI = label.fit_transform(data.LastCPI)
data.Distance2 = label.fit_transform(data.Distance2)

data.info()  # below we can see the datatypes have been converted

# Displaying 3D graph showing relation between room values, distance and price values
Rooms = data['Rooms'].values
Type = data['Type'].values
Price = data['Price'].values
Distance = data['Distance'].values
Week = data['Week'].values
CPI = data['CPI'].values
LastCPI = data['LastCPI'].values
Distance2 = data['Distance2'].values

from mpl_toolkits.mplot3d import Axes3D
# Plotting the scores as scatter plot
fig = plt.figure()
ax = Axes3D(fig)
ax.scatter(Rooms, Distance, Price, color = '#ef1234')
plt.show()

dataset = pd.concat([pd.Series(1, index=data.index, name='Bias'), data], axis=1) #adding "Bias" column with 1 as the data
dataset.head() #printing the dataset after adding the bias term

dataset.isnull()

# df = pd.DataFrame(data)
# df = data.astype({'Rooms':'float64','Type':'float64','Price':'float64','Distance':'float64','Week':'float64','CPI':'float64','LastCPI':'float64','Distance2':'float64'})

"""##**Division of independent and dependent variable**"""

x = data.iloc[:,[0,1,2,4,5,6,7]]
y = data.iloc[:,2]

# Xcol = data.iloc[:,[0,1,3,4,5,6,7]]
# Ycol = data.iloc[:,2]

"""##**Calculatiang Beta**"""

beta = np.array([0]*len(x.columns))

"""##**hypothesis function is defined**"""

m = len(data) # getting the length of the dataset
def hypothesis(beta, X):
  return beta*x

"""##**Cost/Loss Function calculation**
Loss function calculates the Root Mean Squared Error
"""

def loss_function(X, Y, beta):
  h = hypothesis(beta, x)
  h = np.sum(h, axis=1)
  return sum(np.sqrt((h-y)**2))/(2*m)

"""##**Defining Gradient Descent Algorithm**"""

def gradientDescent(X, y, beta, alpha, i):
  # initializing an array to store the cost/loss function values in each iteration,
  # so that we can visualize the data later
  J = []
  k = 0 # initializing iteration count
  
  while k < i:
    h = hypothesis(beta, x) # helps in predicting the target variable h
    h = np.sum(h, axis=1) # the response variable h in each iteration
    
    for c in range(0, len(x.columns)): # for the optimization of beta value for all the X values
      beta[c] = beta[c] - alpha*(sum((h-y)*x.iloc[:,c])/len(x)) # applying beta optimization formula
    j = loss_function(x, y, beta) # calculating the loss function
    J.append(j) # storing the loss function
    k += 1
  return J, j, beta # retuns the loss function values and the optimized beta value

"""##**Division of training data and test data**"""

# Xtrain, Xtest, Ytrain, Ytest = train_test_split( x, y, test_size = 0.3,random_state=42)
Xtrain, Xtest, Ytrain, Ytest = train_test_split( x, y, train_size = 0.7,random_state=42)

Xtest.shape # finding out the rows and columns of the test dataset, so that they can be plotted later.

"""##**Applying Gradient Descent Algorithm**"""

J, j, beta = gradientDescent(Xtrain, Ytrain, beta, 0.05, 5000)

Xtrain=np.arange(0,len(Xtrain),1)

Xtest=np.arange(0,len(Xtest),1)

"""##**Calculation of Predicted Values**"""

y_pred = hypothesis(beta, Xtest) # getting the product of X and beta values
y_pred = np.sum(y_pred, axis=1) # summation of all the products to get the predicted Y value

"""Scatter Plot"""

plt.figure()
plt.scatter(x=list(range(0, 2815)),y = Ytest, color='blue') # plotting the actual values in blue
plt.scatter(x=list(range(0, 2815)),y = y_pred, color='green') # plotting the predicted values in black
plt.show()

"""##Calculating root squared score"""

def r2score(y_pred, y):
    rss = np.sum((y_pred - y) ** 2)
    tss = np.sum((y-y.mean()) ** 2)
    
    r2 = 1 - (rss / tss)
    return r2
r2score(y_pred, Ytest)

# rmse = 0
# for i in range(n):
#     y_pred = b0 + b1 * X[i]
#     rmse += (Y[i] - y_pred) ** 2
# rmse = np.sqrt(rmse/n)
# print(rmse)